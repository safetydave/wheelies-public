{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "process_sensor_data.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRS2DhO8KCPj",
        "colab_type": "text"
      },
      "source": [
        "When configured, run all cells in the notebook to process sensor data and extract wheelie events."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDqLJmcUGLwL",
        "colab_type": "text"
      },
      "source": [
        "# Configure Sensor Data Processing\n",
        "\n",
        "These are the main configuration parameters for processing. Some of them will be auto-detected in future."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyE2veLtg8Zq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Note the sensor client is assumed to be the SensorLog app on iOS\n",
        "\n",
        "# function to determine if the front wheel is up in a wheelie\n",
        "def wheel_up(pitch):\n",
        "  return pitch > 0.88\n",
        "\n",
        "# location where log files have been uploaded\n",
        "UPLOAD_FOLDER = 'My Drive/wheelie_training/'\n",
        "DELTA_ONLY = True\n",
        "\n",
        "# define a default sample rate and any exceptions\n",
        "DEFAULT_SAMPLE_RATE = 10\n",
        "custom_sample_rate = {'2020-07-07_7_55_08.csv': 100,\n",
        "                      '2020-07-07_7_57_52.csv': 100,\n",
        "                      '2020-07-07_8_01_14.csv': 100,\n",
        "                      '2020-07-07_8_05_51.csv': 100}\n",
        "\n",
        "# this configuration for adding distance data to wheelies\n",
        "# latitude_in_degrees is used to calculate distance from lat, long\n",
        "latitude_in_degrees = -38\n",
        "fill_samples_columns = ['locationTimestamp_since1970',\n",
        "                        'locationLatitude',\n",
        "                        'locationLongitude',\n",
        "                        'locationAltitude',\n",
        "                        'locationSpeed',\n",
        "                        'locationCourse',\n",
        "                        'locationVerticalAccuracy',\n",
        "                        'locationHorizontalAccuracy',\n",
        "                        'locationFloor',\n",
        "                        'locationHeadingTimestamp_since1970',\n",
        "                        'locationHeadingX',\n",
        "                        'locationHeadingY',\n",
        "                        'locationHeadingZ',\n",
        "                        'locationTrueHeading',\n",
        "                        'locationMagneticHeading',\n",
        "                        'locationHeadingAccuracy']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upTb1HlIqQKV",
        "colab_type": "text"
      },
      "source": [
        "# Setup File Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmYOMcfoGIQu",
        "colab_type": "text"
      },
      "source": [
        "Caution changing these file processing parameters "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWYVZHosFvAk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GDRIVE_ROOT = '/content/gdrive/'\n",
        "FS_ROOT = GDRIVE_ROOT + UPLOAD_FOLDER\n",
        "RAW_ROOT = FS_ROOT\n",
        "CSV_EXT = '*.csv'\n",
        "PROC_ROOT = RAW_ROOT + 'processed/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9lAKept2KvZ",
        "colab_type": "text"
      },
      "source": [
        "Mount Google Drive to read and write data files. You will be asked to autheticate and authorise this notebook to access your files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaZjK4jGRhDT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(GDRIVE_ROOT)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22BNJYGKhExJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "!pip install python-rle\n",
        "import rle\n",
        "\n",
        "def get_df(fname):\n",
        "  return pd.read_csv(fname)\n",
        "\n",
        "def put_df(df, fname):\n",
        "  df.to_csv(fname)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfH4LjDMrruu",
        "colab_type": "text"
      },
      "source": [
        "# Process Sensor Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vp1W162tnH9R",
        "colab_type": "text"
      },
      "source": [
        "These are utilities for adding location and distance data to wheelie events"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dr1uTkn9GAKY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "\n",
        "EARTH_RADIUS = 6371000\n",
        "distance_per_lat = EARTH_RADIUS * math.pi / 180\n",
        "distance_per_lng = distance_per_lat * math.cos(latitude_in_degrees * math.pi / 180)\n",
        "\n",
        "def fill_timeseries(ts_df):\n",
        "  for f in fill_samples_columns:\n",
        "    ts_df[f].interpolate(inplace=True)\n",
        "\n",
        "def calculate_distance(df):\n",
        "  lat_dist = (df['end_lat'] - df['start_lat']) * distance_per_lat\n",
        "  lng_dist = (df['end_lng'] - df['start_lng']) * distance_per_lng\n",
        "  return np.hypot(lat_dist, lng_dist)\n",
        "\n",
        "def process_extended_data(ts_df, event_df):\n",
        "  ext_src = ['locationLatitude', 'locationLongitude', 'locationSpeed']\n",
        "  ext_tgt = ['lat', 'lng', 'speed']\n",
        "  for i, m in enumerate(ext_src):\n",
        "    event_df['start_' + ext_tgt[i]] = map_event_data(ts_df, m, event_df)\n",
        "    event_df['end_' + ext_tgt[i]] = map_event_data(ts_df, m, event_df, end=True)\n",
        "  event_df['distance'] = calculate_distance(event_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsE4iUGinKjw",
        "colab_type": "text"
      },
      "source": [
        "This is the core event transformation from timeseries data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVHgugChF_RX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode_events(event_label, sample_rate):\n",
        "  # runs of records with the same event label\n",
        "  runs = list(rle.encode(event_label))\n",
        "  data = np.transpose(np.array(runs))\n",
        "  df = pd.DataFrame(data, columns=['state', 'run_length'])\n",
        "  run_end = df['run_length'].cumsum()\n",
        "  df['run_start'] = run_end - df['run_length']\n",
        "  # calculate timings\n",
        "  df['start_time'] = df['run_start'] / sample_rate\n",
        "  df['duration'] = df['run_length'] / sample_rate\n",
        "  return df[df['state'] == 1]\n",
        "\n",
        "def map_event_data(ts_df, column, event_df, end=False):\n",
        "  # map data from the start [default] or end of an event\n",
        "  select = event_df['run_start']\n",
        "  if end:\n",
        "    select = event_df['run_start'] + event_df['run_length']\n",
        "  return list(ts_df[column][select]).copy()\n",
        "\n",
        "def process_ts_df(ts_df, sample_rate):\n",
        "  # get time series data\n",
        "  fill_timeseries(ts_df)\n",
        "  ts_df['wheel_up'] = ts_df['motionPitch'].apply(wheel_up).apply(int)\n",
        "  # extract events\n",
        "  event_df = encode_events(ts_df['wheel_up'], sample_rate)\n",
        "  # map event timestamps\n",
        "  timestamp = map_event_data(ts_df, 'loggingTime', event_df)\n",
        "  event_df.insert(0, 'timestamp', timestamp)\n",
        "  # further data\n",
        "  process_extended_data(ts_df, event_df)\n",
        "  return event_df  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X498hUxBJySC",
        "colab_type": "text"
      },
      "source": [
        "This will process all files in the folder, writing processed versions of the time series data (with missing values, etc) and a log file of all wheelie events."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQp57ji_LkKr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "import os.path\n",
        "from pathlib import Path\n",
        "\n",
        "Path(PROC_ROOT).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "raw = sorted(glob.glob(RAW_ROOT + CSV_EXT))\n",
        "\n",
        "accum = pd.DataFrame()\n",
        "accum_fname = PROC_ROOT + 'accum.csv'\n",
        "if DELTA_ONLY:\n",
        "  accum = get_df(accum_fname)\n",
        "\n",
        "for f in raw:\n",
        "  # get timeseries data per file\n",
        "  f_file = f.split('/')[-1]\n",
        "  proc_fname = PROC_ROOT + f_file\n",
        "  if DELTA_ONLY and os.path.isfile(proc_fname):\n",
        "    continue\n",
        "  print('Processing {} ...'.format(proc_fname))\n",
        "  sample_rate = DEFAULT_SAMPLE_RATE\n",
        "  if f_file in custom_sample_rate:\n",
        "    sample_rate = custom_sample_rate[f_file]\n",
        "  ts_df = get_df(f)\n",
        "  \n",
        "  # accumulate events\n",
        "  events = process_ts_df(ts_df, sample_rate)\n",
        "  events['source'] = [f] * len(events.index)\n",
        "  accum = accum.append(events)\n",
        "  \n",
        "  # write changes to ts_df\n",
        "  put_df(ts_df, PROC_ROOT + f_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBsZlno98Xww",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "accum.index = pd.Series(range(len(accum.index)), name='wnum')\n",
        "put_df(accum, PROC_ROOT + 'accum.csv')\n",
        "print('Total {} wheely events'.format(len(accum.index)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCDYR7gRUMTM",
        "colab_type": "text"
      },
      "source": [
        "# Handy Summary Statistics\n",
        "\n",
        "Describe and plot effort (number of wheelies attempted) and results (median and max time and distance) against days of training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usPF0eRp7m2e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_summary_time(df):\n",
        "  count = len(df.index)\n",
        "  total = df['duration'].sum()\n",
        "  max = df['duration'].max()\n",
        "  median = df['duration'].median()\n",
        "  return count, total, max, median\n",
        "\n",
        "def get_summary_dist(df):\n",
        "  count = len(df.index)\n",
        "  total = df['distance'].sum()\n",
        "  max = df['distance'].max()\n",
        "  median = df['distance'].median()\n",
        "  return count, total, max, median\n",
        "  \n",
        "count, total, max, median = get_summary_time(accum)\n",
        "print('{} wheelies'.format(count))\n",
        "print('total duration {:.1f}s'.format(total))\n",
        "print('longest duration {:.1f}s'.format(max))\n",
        "print('median duration {:.1f}s'.format(median))\n",
        "count, total, max, median = get_summary_dist(accum)\n",
        "print('total distance {:.1f}m'.format(total))\n",
        "print('longest distance {:.1f}m'.format(max))\n",
        "print('median distance {:.1f}m'.format(median))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6XCI_GBQYzt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "\n",
        "accum['datetime'] = accum['timestamp'].apply(datetime.datetime.fromtimestamp)\n",
        "day_grouped = accum.groupby(pd.Grouper(key='datetime', freq='1D'))\n",
        "groups = day_grouped.groups\n",
        "for g in groups.copy():\n",
        "  try:\n",
        "    day_grouped.get_group(g)\n",
        "  except:\n",
        "    groups.pop(g)\n",
        "\n",
        "gs_data_time = [list(get_summary_time(day_grouped.get_group(g))) for g in groups]\n",
        "ds_df_time = pd.DataFrame(gs_data_time, columns=['count', 'total time', 'max', 'median'])\n",
        "gs_data_dist = [list(get_summary_dist(day_grouped.get_group(g))) for g in groups]\n",
        "ds_df_dist = pd.DataFrame(gs_data_dist, columns=['count', 'total dist', 'max', 'median'])\n",
        "\n",
        "fig, ax = plt.subplots(1, 2)\n",
        "fig.set_figwidth(12)\n",
        "fig.suptitle('Wheelie Summary')\n",
        "\n",
        "ax[0].plot([g for g in groups], ds_df_dist['count'].cumsum(), '-o')\n",
        "ax[0].set_ylabel('cumulative wheelies')\n",
        "ax[0].set_title('Effort')\n",
        "ax[0].tick_params(axis='x', labelrotation=30)\n",
        "\n",
        "ax[1].plot([g for g in groups], ds_df_time['max'].expanding().max() * 4, '--', color='tab:blue')\n",
        "ax[1].plot([g for g in groups], ds_df_time['max'] * 4, '-o', color='tab:blue')\n",
        "ax[1].plot([g for g in groups], ds_df_dist['max'].expanding().max(), '--', color='tab:orange')\n",
        "ax[1].plot([g for g in groups], ds_df_dist['max'], '-o', color='tab:orange')\n",
        "ax[1].plot([g for g in groups], ds_df_time['median'].expanding().max() * 4, '--', color='tab:green')\n",
        "ax[1].plot([g for g in groups], ds_df_time['median'] * 4, '-o', color='tab:green')\n",
        "ax[1].plot([g for g in groups], ds_df_dist['median'].expanding().max(), '--', color='tab:red')\n",
        "ax[1].plot([g for g in groups], ds_df_dist['median'], '-o', color='tab:red')\n",
        "ax[1].legend(['time max (max)', 'time max',\n",
        "            'dist max (max)', 'dist max',\n",
        "            'time med (max)', 'time med',\n",
        "            'dist med (max)', 'dist med'],\n",
        "           loc='right', bbox_to_anchor=(1.6, 0.7))\n",
        "ax[1].set_ylabel('distance (m)')\n",
        "ax[1].set_title('Results')\n",
        "ax[1].tick_params(axis='x', labelrotation=30)\n",
        "\n",
        "secaxy = ax[1].secondary_yaxis('right', functions=(lambda x: x / 4, lambda x: x * 4))\n",
        "secaxy.set_ylabel('time (s)')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
